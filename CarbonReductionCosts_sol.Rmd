# Problem Set: Estimating CO2 Reduction Costs

Author: Daniel Dreyer

#< ignore
```{r ""}
library(restorepoint)
# facilitates error detection
# set.restore.point.options(display.restore.point=TRUE)
library(RTutor)
library(yaml)
#library(restorepoint)
setwd("X:/libraries/RTutor_BA")
ps.name = "CarbonReductionCosts"; sol.file = paste0(ps.name,"_sol.Rmd")
libs = c("dplyr", "ggplot2", "gridExtra", "stargazer", "tidyverse", "ggalt",  "splines", "kableExtra", "scatterplot3d", "RColorBrewer") # character vector of all packages you load in the problem set
name.rmd.chunks(sol.file) # set auto chunk names in this file
create.ps(sol.file=sol.file, ps.name=ps.name, user.name=NULL,libs=libs, stop.when.finished=FALSE, addons="quiz", extra.code.file="dp.R")
show.shiny.ps(ps.name, load.sav=FALSE,  sample.solution=TRUE, is.solved=TRUE, catch.errors=TRUE, launch.browser=TRUE)
stop.without.error()
rtutor.package.skel(sol.file=sol.file, ps.name=ps.name,libs=libs,
                    pkg.name="RTutorCarbonReductionCosts",   # Name of the problem set package
                    pkg.parent.dir = "X:/libraries/RTutor_BA", # Parent directory 
                    author="Daniel Dreyer", # Your name
                    github.user="daniel.dreyer",     # Your github user name
                    extra.code.file="dp.R", # name of extra.code.file
                    var.txt.file="variables.txt",    # name of var.txt.file
                    overwrite=FALSE  # Do you want to override if package directory exists?
)
```
#>

<style>
img {
    display: block;
    margin-left: auto;
    margin-right: auto;
    max-width: 100%;
}
</style>

Welcome to this `Interactive Problem Set`. During this problem set we will examine the effect of carbon taxes on emissions in the electricity sector. The analysis is based on **"Inferring Carbon Abatement Costs in Electricity Markets: A Revealed Preference Approach using the Shale Revolution"** by Joseph A. Cullen, Erin T. Mansur (2016) - further referred as Cullen. You can downlaod the paper including the author's STATA code from: <a href="https://www.aeaweb.org/articles?id=10.1257/pol.20150388   target = "_blank"> https://www.aeaweb.org/articles?id=10.1257/pol.20150388</a>

____ ergebnisse ____

We will use the statistical programming language `R` to replicate the analyisis proposed by Cullen. I will provide information about concepts and functions along the way, but you are required to have basic knowledge of `Statistics` and `R`. If you are completely new to programming in `R` you should't feel left behind though. Understanding the basic concepts is pretty straight forward, you can find a useful beginners guide [here](https://cran.r-project.org/manuals.html). Below you will find an overview of content and a guideline on how to solve upcoming interactive problems.  

## Exercise Content


$\qquad$ *Exercise 1* - Motivation (rename later)

$\qquad$ *Exercise 2* - TBA

$\qquad$ *Exercise 3* - TBA

$\qquad$ *Exercise 4* - TBA

$\qquad$ *Exercise 5* - TBA


### Instructions


The problem set offers different ways of interaction. Some provide you additional information or test your knowledge, others require you to write your own code snippets. Coding exercises are marked as **TASK**. You are not required to solve exercises in the given order but its recommended. Below you can read find the different types of interactions with their corresponding functions:   

 - *Info Boxes*: Contain additional information on technical terms or documentation of functions.
 
 - *Quizzes*: Evaluate your knownledge of topics before we dive in our analysis.
 
 - *Code Chunks*: Require you to complete small parts of Code. The work flow of solving code chunks is intuitive and as followed:
                
  + `edit` : At the start of each exercise click check to start editing the Code chunk.
  + `run chunk`: Runs the chunk and displays outputs in the corresponding console.
  + `check`: Check your input against the solution.
  + `hint` : You can request a hint if you have problem solving a task.  
&nbsp;

  
  Additional functions:    
                   
  + `data`: Redirects you to the data browser - you can navigate through the data set.
  + `solution` : Displays the solution of the task.  
&nbsp;


After finishing a exercise, click `Go to next exercise` at the bottom or navigate around with the help of the bar on top.


## Exercise 1 -- Insights into Energy Markets

In order to meet the requirement under the Paris Agreement to be climate-neutral by the year 2050, the world has to become more energy efficient. Right now the world's energy consumption keeps rising daily and even though new technologies are being developed to produce more `green` electricity, we are still heavily dependant on `fossil fuels`. `Coal` is hereby one of the most important sources of energy, but at the same time the biggest source of carbon emissions. Reason being that it is easy to store, transport, doesn't alter and is mined in huge quantities all around the world. Given that coal supplies will last for centuries, it is a major goal to find incentitives to reduce it's use and therefore $CO_2$ `emissions`.
Another major energy source is `natural gas`, which is obviously also a type of fossil fuel, but it's $CO_2$ emissions and heat density are far more efficient. Nevertheless, natural gas wasn't considered to be an real alternative to coal because of higher costs through more difficult extracting methods and handling.  
This changed due to the `Shale Revolution`, which started in the early 2000s in the United States. Natural gas began to not only be a by-product of the oil industry, but could now be extracted in large quantities and in a targeted manner. Lanfrancois (2012) estimates, that based on the `Clean Power Plan` introduced by the Obama Administration in 2015, $CO_2$ emissions from the electricity sector could be reduced by roughly 23 to 42 percent by replacing existing coal to gas fired generators.    
  
#< info "Shale-Revolution"
Shale gas is a form of natural gas and is extracted from shale formation, a technology known as fracking. Since the start of this century, shale gas became a increasingly more popular form of natural gas in the United States. Whereas in the year 2000 shale gas only provided 1% of extracted natural gas in the U.S., nowadays it makes up roughly 50%. The long term effects on the environment however are highly uncertain and heavily debated.  

You can find further information on this topic below:  
<a href="https://en.wikipedia.org/wiki/Shale_ga   target = "_blank"> https://en.wikipedia.org/wiki/Shale_gas</a>s  
<a href="https://en.wikipedia.org/wiki/Hydraulic_fracturing   target = "_blank"> https://en.wikipedia.org/wiki/Hydraulic_fracturing</a>
#>
  
Let's start with some interactive tasks. In `Exercise 1` we will get insights into global energy markets and explore time-series data of historic prices and generated electricity.

**Task:**
Use the `read_csv` function to load the data set `exercise1.csv` and store it in a variable called `dat`. `Read_csv` reports the variable type of each column after execution. This will come handy later on in this exercise. In future exercises we will disable this message.  

#< info "read.csv() and write.csv()"
The command `read_csv()` reads in a **csv file** and stores it into a given variable. Csv is a format for data, that uses commas as seperators and periods as decimals. Another version for data sets are **csv2 files**, where semicolons are used for seperators and commas for decimals. We will only use csv files in this problem set.

Given you want to load a file, which is in the same working directory, you can use the command below:

```{r "1",eval=FALSE}
example <- read_csv("example.csv")
```

If the file is saved outside of your working directory you have to differentiate between two possibilities.  
For files that are "above" your current working directory, you have to provide the full path to the file. You can view your working directory with `getwd()`:

```{r "1__2",eval=FALSE}
example <- read_csv("C:/Data/example.csv")
```

For files that are located "below" the working directory it is sufficient to specify the file path starting with your current working directory:

```{r "1__3",eval=FALSE}
example <- read_csv("./Data/example.csv")
```

Csv files can be saved using the `write_csv` command:

```{r "1__4",eval=FALSE}
write_csv(example, file="example.csv")
```

For further information, use `help(write_csv)`.
#>

```{r "1__5"}
#< task
# ___ <- read_csv("./Data/exercise1.csv")
#>
dat <- read_csv("./Data/exercise1.csv")
#< hint
display("Replace the variable name with the one given in the exercise description. No other changes are needed.")
#>
```

**Task:** The first thing we do after loading a new data set is looking at it's structure. Use the `head()` command to display the first **eight** rows of the data set `dat`. When not given a second argument, `head()` displays the first 5 rows on default. Alternatively, the problem set provides a seperate tab. You can click on `data` and get redirected to the `Data Explorer` tab, where you can navigate around the data frame and see some basic statistics.

```{r "1__6"}
#< task
# head(dat, ...)
#>
head(dat, 8)
#< hint
display("The second input parameter defines the number of row that are shown. Replace ... with the number from the task description.")
#>
```

The data frame includes 214 observations of **monthly** data from the year 2002 to 2019. It contains generated power in the U.S. as well as prices of `natural gas` in Europe and the United States. Keep in mind that different commodities are traded in different units, coal in metric tons (`\$/mt`) and gas in million British thermal units (`\$/mmBTU`). 
I mentioned that it would come handy to know which data types your columns inherit (hovering over the column names in the output above will give you the same information). `R` requires `dates` to be stored as type `Date` to be able to create plots along a time line. Since the date column is currently stored as type `character` we have to convert the variable into a `date format` by using `as.Date()`.

**Task:**  Transform the data type with `as.Date()` and save it to the same column.
```{r "1__7"}
#< task
# ... <- ...(dat$date, format="%m/%d/%Y")
#>
dat$date <- as.Date(dat$date, format="%m/%d/%Y")
#< hint
display("Convert the column dat$date into a date format using as.Date().")
#>
```

Now that we imported our data set and processed it for analysis, let's start exploring. First, we create a plot that tells us more about the price development of gas in both regions. Because of the fact that the `Shale-Revolution` started in the beginning of this century, we expect a increasing supply and therefore falling prices for gas in the United States. Since commodity markets are split into regions, the European Prices should somehow differ.  
Besides funtionality that `R` provides on default, there are several packages that let us create highly customizable plots. We will mostly use the package `ggplot2`, which is widely considered to be one of the most powerful packages to create plots, having the downside that the handling can be a bit tricky. Therefore, you aren't required to create entire plots on your own but but you will fill in gaps. This way you get used to the logic behind `ggplot2` and at the same time would't be frustrated if your output didn't meet the exact requirements.  
  
**Task:** Use the data frame `dat` to plot `historic prices` of natural gas in `Europe` and the `United States` along `date` on the x-Axis. Just press *check*.

#< info "ggplot2"
The package `ggplot2` is a powerful data visualization package for `R`, which is part of the `tidyverse` environment. Besides basic functions that are also provided by native `R`, it allows the user to create highly customizable graphs by **altering**, **adding** or **removing** components.

You are not required to have deeper knowledge about the functionality of `ggplot2`. However, you can find further information [here](https://ggplot2.tidyverse.org/) or type `help(ggplot2)`.
#>
```{r "1__8",fig.width=9, fig.height=6, dev='svg'}
#< task
#ggplot(data=___, aes(x=___)) +
#  geom_line(aes(y=dat$price_naturalgas_USA, color="United States"), size=1) +
#  geom_line(aes(y=dat$price_naturalgas_Europe, color="Europe"), size=1) +
#  labs(x = "Year", y = "", title = "Natural Gas Prices", subtitle = "Y=$/mmBTU")+
#  scale_color_manual(name="Region",values = c("blue","red"))
#>
ggplot(data=dat, aes(x=date)) +
  geom_line(aes(y=dat$price_naturalgas_USA, color="United States"), size=1) +
  geom_line(aes(y=dat$price_naturalgas_Europe, color="Europe"), size=1) +
  labs(x = "Year", y = "", title = "Natural Gas Prices", subtitle = "Y=$/mmBTU")+
  scale_color_manual(name="Region",values = c("blue","red"))
```

Looking at the plot, our assumptions are confirmed. The spikes in 2005 and 2008 are mostly consequences of the Iraq War and the Financial Crisis. Apart from that, we observe decreasing gas prices in the U.S. from just over \$12 in 2008 to under \$2 in 2016. Since then the prices remained at roughly the same level.  
Natural gas prices in Europe have remained at a significantly higher level. One reason being, that fracking isn't as popular in Europe, therefore a major part of gas gets imported from Russia which drives prices. Another reason is that Europe has a widely different energy mix in comparisn to the United States. According to the European Environment Agency (EEA), the consumption of gas decreased in average by 1.4% per year since 2005, whereas in the U.S. natural gas reached new all-time highs almost every year. 

Besides information of fuel prices, our data frame also provides data for generated power. Before we take a look at the time-series for his, take your first quiz:  

#< quiz "Proportion of Sectors"
question: Did the proportion of gas-generated electicity has increased or decreased over time in comparison to coal-generated electricity ?

sc:
    - increased*
    - decreased
success: Great, your answer is correct!
failure: Try again.
#>  
&nbsp;

**Task:** Create a new `ggplot2` for generated power. Plot the `date` on the x-Axis and the generated electricity on the y-Axis. We want seperate line for `gas generated power` as well as `coal generated power`, which can be added with `geom_line`. For reference you can look at the code from the previous graph or conduct the `help()` function.

```{r "1__9",fig.width=9, fig.height=6, dev='svg'}
#< task
#ggplot(data=___, aes(x=___)) +
#  ____(aes(_=dat$generated_coal, color="Coal Generation (TWh)"), size=1, alpha=0.5) +
#  ____(aes(_=dat$generated_gas, color="Natural Gas Generation (TWh)"), size=1, alpha=0.5) +
#  labs(x = "Year", y = "", title = "Monthly Generation by Fueltype", subtitle = "Y=Monthly Electricty (TWh)")+
#  scale_color_manual(name="",values = c("black","red"))+
#  scale_y_continuous(breaks=seq(0,200,25))
ggplot(data=dat, aes(x=date)) +
  geom_line(aes(y=dat$generated_coal, color="Coal Generation (TWh)"), size=1, alpha=0.5) +
  geom_line(aes(y=dat$generated_gas, color="Natural Gas Generation (TWh)"), size=1, alpha=0.5) +
  labs(x = "Year", y = "", title = "Monthly Generation by Fueltype", subtitle = "Y=Monthly Electricty (TWh)")+
  scale_color_manual(name="",values = c("black","red"))+
  scale_y_continuous(breaks=seq(0,200,25))
#>
```

The results are in line with what we should expect. With a higher supply of gas and decreasing prices, gas plants increased their share in power generation over time. Giving you a bit more background information: Coal-fired plants have lower operating costs than gas-fired generators, but are expensive to start and slow to adjust to fluctuating demands. Gas-generators can fill these gaps. There are two different types of gas-generators, one being `peaker plants`. They run in high demand hours due to their fast start-up times, but on the downside suffer from high marginal costs. The second type are `combined cycle gas turbines` (CCGT). They have low heat rates (high efficiency of turning fuel into power) and are used to provide baseline power generation throughout the day.  
Combining the factors above, we can say that both fuel-generators have the possiblity to generate power in all situation and meet our needs, meaning that the fuels are `switchable`. However, the mechanism of `fuel switching` is a lot more complex than just about the cost efficiency. We already mentioned some factors above, but capacity of plants, transmission grid limits (Mansur & White 2012, Davis & Hausman 2015) or firms market power (Bushnell, Mansur & Saravia 2008) also play a big role.    
To wrap this up, we observe severe fluctuations of generated power along the year, that occur with a smaller peak in summer and a bigger one in the winter season. These fluctuation are largely driven by using air conditioning in summer and space heating in winter (EIA, "Today in Energy", 2020/3).  
  
#< award "Artist"
You created your first plots! You will earn more awards throughout the problem set. After you completed all exercises you will see how many awards you achieved.
#>

In the next exercise we will set a connection between `fuel-switching` and how we could use this for our analysis. Click `Go to next exercise` to continue.



## Exercise 2 -- Theory

### Relationship between Fuel and Carbon Prices

In this exercise we pick up the mechanism of `fuel switching` and try to use it for our analysis. We already mentioned that the usage of certain generators isn't purely limited by cost efficiency. If we assume that we aren't limited by the factors we mentioned at the end of `Exercise 1`, `Marginal Costs` will play a big role in deciding what fuel-generator we use for power generation. Marginal costs are per definition not only the fuel prices itself, but include all costs that are needed to produce the additional MWh of power. 
Cullen hereby developed a model which builds upon the idea, that we can switch freely between fuel types and that prices and emission are in one way or another correlated. With that in mind we define the `Marginal Costs` of fossil-powered plants as a Equation of `heat rate` and the `costs of burning fuel`. Variable names of this exercise are explained in the info-box below:

$$\tag{1}MC=HR\cdot(P_{fuel}+CO_{2,fuel}\cdot P_{co2})=HR\cdot C_{fuel}$$

#< info "Variable names in this exercise"
$MC$: Marginal Costs  
$HR$: Heat Rate, mmBTU/MWh  
$C_{coal}$: Cost of burning coal  
$C_{gas}$: Cost of burning gas  
$P_{coal}$: Coal Price, $/mmBTU  
$P_{gas}$: Gas Price, $/mmBTU  
$CO_{2,coal}$: Carbon content of coal, tons/mmBTU  
$CO_{2,gas}$: Carbon content of gas, tons/mmBTU  
$P_{co2}$: Carbon Price , $/ton
#>

In `Exercise 1` we observed amounts of generated electricity for `coal` and `gas` and their variance over time. Because of the fact that the cost efficiency of coal is generally better, we have to introduce a incentive to reduce the use of coal in relation to gas. One way to introdude a change in cost efficiency is to propose `carbon prices`. This is especially viable in this case because coal contains approximately `twice` as much $CO_2$ as natural gas and therefore the cost efficiency is more effected. Another factor that drives this momentum is that gas generators generally have a more efficient heat rate than coal generators (as we metioned at the end of last exercise).  
Combining these two facts lead to `steeper marginal cost` for coal generators when carbon prices are introduced. Reflecting this to our upcoming analysis, we won't observe any carbon prices, but a time-series of fuel prices that are transformed into costs of burning fuel with `Equation 1`. The relative cost between coal and gas is therefore the ratio of the two:

$$\tag{2}costratio=\frac{C_{coal}}{C_{gas}}$$

By combining Equation `1` with the one above, we can explain `cost ratios` as a function of `fuel prices`, `carbon content` and `carbon prices`:

$$\tag{3}costratio=\frac{C_{coal}}{C_{gas}}=\frac{P_{coal}+CO_{2,coal}\cdot P_{co2}}{P_{gas}+CO_{2,gas}\cdot P_{co2}}$$

The values we use in our analysis for carbon content $CO_{2,coal}$ and  fuel prices $P_{fuel}$ will be fixed. Carbon content is reported by the `EIA` and fuel prices are predictions based on the year 2025. The values are listed below:  

- Average delivered coal price `$2.25/mmBTU` and gas prices `$5.75/mmBTU` (forcast for 2025)
- Carbon content `Natural Gas`: 117 lbs carbon/MMBTU or `0.0585 tons/MMBTU`  
- Carbon content `Coal`: 210.8 lbs carbon/MMBTU or `0.1054 tons/MMBTU`  

`Equations 2` and `3` are hereby central ideas that will lead to our final results. By interpreting these functions, we conclude, that the `cost ratio` will rise for higher `costs of burning coal` or lower `costs of burning gas`. Since this is a central idea in our analysisi, lets visualize the relationship:

<img src="./Material/relationship.png" alt="drawing" width="600"/>
Source: Cullen (2016)  
  
Panel `(a)` shows the relationship between fixed costs of `coal` and `gas` when `carbon prices` get introduced. As we expalained above, we observe higher marginal costs for coal and at a certain level, gas has lower marginal costs or in other words becomes more cost efficient. Panel `(c)` displays the results when transforming fuel prices in cost ratios as defined in `Equation 2`. We can observe the same in absence of carbon prices with fixed coal prices and variable gas prices as seen in panel `(b)` and `(d)`. 

#< quiz "carbon prices"
question: Based on the graph above, can we create any cost ratio under the assumption that we introduce carbon prices under fixed fuel prices?
sc:
    - Yes*
    - No
success: Great, your answer is correct!
failure: Try again.
#>  
&nbsp;

The answer is the central idea to our analysis. We can create every variation of cost ratios, either by introducing `carbon prices` to fixed fuel prices, or without carbon prices by varying costs of `coal` and `gas`. We use this variation in the cost ratios observed in our data to understand how emissions change when gas generators become more competitive with coal plants (Cullen 2016). 

$$\tag{4}{P_{co2}}=\frac{costratio\cdot{P_{gas}}-{P_{coal}}}{CO_{2,coal}-costratio\cdot CO_{2,gas}}$$

We will use this equation in the second part of the analysis (`4.2`) to transform our fuel costs into carbon prices. Therefore we will be able to predict the impact of carbon taxes on emissions and calculate abatement costs.  
  
<br/><br/>
  
Since it is not common to have electricity grid in Europe that are formed like the ones in the United States, I will introduce the concept of interconnection in the following sub-section.

### Interconnections

<img src="./Material/interconnections.png" alt="drawing" width="600"/>
Source: Cullen (2016)

The American Electric system is made up of three major `interconnections`, which in turn concist of different balancing authorities (responsible for maintaining the electricity balance within the region). Local electricity grids are hereby connected to form a network, which provides higher stablity and reliability. These `interconnections` operate mostly independent from each other and exchange little to no electricity. This is a huge difference to the European electricity grid, where grid stability is ensured across borders.

In the graph below you can see the `3` interconnection we include data of in this problem set. I listed a few details for each below:
- `Eastern Interconnection (EAST)`: Consists of 36 balancing authorities and extends from the East Coast to the Rocky Mountains.
- `Western Interconnection (WECC)`: Involves 37 balancing authorities, which are located in the West of North America.
- `Electric Reliability Council of Texas` (ERCOT): Consists of large parts of Texas

In the first part of this exercise we already hinted that our model will we some form of regression of price ratios against emissions. Since we are essentially observing three different energy markets we should confirm the distribution of our data points. Therefore we create a graph that plots all price ratios within our data set against $CO_2$ emissions. We run the model seperately on each interconnection if find a large distribution across interconnections. We will present the data set for the analysis in detail in upcoming exercises. For not it's enough to get an impression on the distribution.

**Task:** Just press *check*.

#< info "geom_encircle"
`Geom_encircle()` is part of the package `ggalt` which extends functionality of `ggplot2`. Besides standard functionality for plotting points or lines that is provided by basic `ggplot2`, it provides a advanced framework for visualising your data. `Geom_encircle()` automatically encloses points in a polygon and can be used to visualise differences in groups of data.

Call `help(geom_encircle)` to get further information.
#>

 //////////// rename data set

```{r "2",message=FALSE}
#< task_notest
dat <- read_csv("Data/exercise3.csv")
ggplot(dat,aes(x=coalprice/gasprice,y=co2mass/1000,color=intercn))+
        geom_point(alpha=0.2) +
        geom_encircle(aes(group=intercn,fill=intercn),alpha=0.3, s_shape=1) +
        theme_bw()+ 
        labs(y="", 
        x="Price Ratio", 
        subtitle="y=CO2 Emissions in 1000 tons/day",
        title="Distribution of data")
#>
```

As you can the distribution of data point for the `Eastern` interconnection is way different than the other two interconnections. We could probably find a model that fits `ERCOT` and `WECC`, but its unlikely to fit `EAST` as well. Because of this we will perform our analysis on each `interconnection` seperately. 

#< award "Theorist"
You learned about the necessary theory. You are ready to start with the analysis!
#>

In the next chapter we will begin constructing a model that will implement the theory we introduced in this exercise. Click `Go to next exercise`.
































## Exercise 3.1 -- Emission Response Curves - A simple Approach

A short recapture: Up to this point we some insights into the U.S. energy market and introduced the concept of `fuel switching` as well as `interconnection`. In this exercise we will start to combine these findings and develop a model which allows us to predict the impact of changing fuel prices on emissions. We start by proposing a simple linear approach and work our way to more complex but also more precise results.  
If you are purely interested in the final model and the economic impact, you can skip to `Exercise 4.1`. This exercise and the following will focus on regression theory, which will lay a basis to understand the methods we use for our main analysis.

**Task:** To get started, load the data set `exercise3.csv`, press `edit` and `check` afterwards.

```{r "3_1",message=FALSE}
#< task_notest
dat <- read_csv("Data/exercise3.csv")
head(dat,3)
#>
```

The data are gathered from several official U.S. agencies and aggregated to a daily level and seperated by interconnection. Along the way we will extend the data set with additional variables. For now you can find the variables and it's associated units below:

`co2mass`: $CO_2$ Emissions in tons  
`gasprice`: Capacity weighted average daily gas price ($ per million BTU)  
`coalprice`: Price of Coal ($ per million BTU)  
`load`: Daily electricity consumption in MWh  

Due to the size of the original data sets we wont do the data preparation in this problem set but rather provide the data sources and work with the final data set. `Emission` data are measured by the Continuous Emissions Monitoring System (CEMS) of the `Environmental Protection Agency (EPA)`. The U.S. Energy Information Administration (EIA) collects data of `coal prices` in Form 923. Spot prices for `gas` can be found at the `Intercontinental Exchange (ICE)` and data for `electricity consumption` or `load` are provided by the `Federal Energy Regulatory Commission (FERC)` in Form 714.  
As mentioned in `Exercise 2` it makes sense for us to run our model on each `interconnection` seperatly. For this purpose we will filter our data set for interconnection `EAST` and develop the model based on these data. Once we find a fitting model we will run it on each interconnection. For ease of interpretation we convert emissions and load from tons to million tons.  

**Task:** Use the pipe operator `%>%` to combine following tasks: Filter the data set `dat` for interconnection `EAST` and store it in `dat_east`. Additionally, calculate the cost ratio between `coalprice` and `gasprice` according to `Equation 2`.

#< info "Pipe, Select, Filter, Mutate"
The pipe operator `%>%` is a feature provided by the `dplyr` Package. Essentially it allows you to exectute multiple operation on a dataframe at once. Hereby, every pipe operator returns a dataframe and passes it to the next connected function.  
`Select()` filters for certain columns of data, whereas `filter()` does the same for rows.  
If you want to create new columns or alter existing, you can use `mutate()`. There are several more functions to alter your data sets, below you find a handy cheat sheet.
  
```{r "3_1__2",eval=FALSE}
example %>% 
  select(1:5) %>%  # keep certain columns from index 1 to 5  
  mutate(new_column = old_column+1) %>% # create or alter columns   
  filter(columnname=="value") # filter for argument
```

You can find a more detailed cheat sheet [here](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) or use the `help()` function.
#>

```{r "3_1__3"}
#< task
# ___ <- data ___ 
#  mutate(costratio = ___/___,
#         co2mass = co2mass/1000000),
#         load = load/1000000) %>%
#  filter(intercn=="___")
#>
dat_east <- dat %>% 
  mutate(costratio = coalprice/gasprice,
         co2mass = co2mass/1000000,
         load = load/1000000) %>%
  filter(intercn=="EAST")
#< hint
display("Fill in the gaps with variable names given in the exercise description and filter for EAST. Don't forget the pipes.")
#>
```



### Linear Regression

We start by proposing a simple linear regression model. In a mathematical term we can express the relationship as followed:

$$CO_{2t}=\beta_{0}+\beta_{1}\cdot costratio_{t}+\epsilon_t$$

#< info "Linear Regression with lm()"

`Lm()` is part of the `stats` package, which is loaded on default. As shown in the syntax example below, it enables you to regress `y` on the indepedent variables `x1` and `x2`. The model itself can be handled just like every other variable, which can be either saved or used in other functions. Another popular function to solve linear regressions in `R` is `felm()`, that provides further functionality e.g. for including `fixed effects`. In this problem set we will strictly use `lm()`.

```{r "3_1__4",eval=FALSE}
example <- lm(y~x1+x2, data=dat_east)
```

You can find more information on the `lm()` function [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/lm) or use the `help()` function.
#>

**Task:** Run a regression with `co2mass` as the dependent variable and `costratio` as independent variable. Store it in the variable `fit1`. 
```{r "3_1__5"}
#< task
# ... <- lm(...~... , data=dat_east)
#>
fit <- lm(co2mass ~ costratio, data=dat_east)
```

**Task:** To show the statistics of our regression we will use stargazer(). We won't need every information that is displayed by the default function. Therefore, we will define our own funtion of stargazer and at the same time introduce custom functions which will be useful later on. Read the `Info Boxes` to find out more about `stargazer` and `custom functions`. Just press **check** to define the functiom.  

#< info "Stargazer"
`stargazer` provides specialised `HTML` formatting for regression tables and summary statistics. It is easy to use, supports a large number of model types and formats data in a more pleasing way. The basic function is called by `stargazer()` and but can be customized heavily. For further information: `help(stargazer)` or  [here](https://cran.r-project.org/web/packages/stargazer/).
#>

#< info "Custom functions"
`R` provides an easy framework to add your own functions. The syntax is quite similar to other programming languages you could be familiar with and you can use these functions as long as you are in the same session. Below you find the basic structure and an example:

```{r "3_1__6",eval=FALSE}
`myfunction <- function(arg1, arg2, ... ){`  
`statements`  
`return(object)}`
```  
Here's an simple functions that adds `5` to the input:
```{r "3_1__7",eval=FALSE}
`example <- function(x) {`  
  `x + 5`  
`}`  
`example(1)`
```  
You find more detailed information [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/function).
#>

```{r "3_1__8"}
#< task_notest
show.regression= function(...){
  library(stargazer)
    stargazer(..., 
            type = "text", 
            style = "aer",  
            digits = 3,
            df = FALSE,
            report = "vct*",
            star.cutoffs = c(0.05, 0.01, 0.001),
            model.names = FALSE,
            object.names = TRUE,
            model.numbers = FALSE, 
            omit.stat=c("f", "ser")
    )
}
#>
```

#< quiz "Reg1"
question: Before taking a look at the summary, how do changing cost ratios effect emissions?

sc:
    - With increasing cost ratios emissions will increase.
    - With increasing cost ratios emissions will fall.*
success: Great, your answer is correct!
failure: Try again.
#>  
  
**Task:** Use the function we defined above to display the summary statistics of `fit1`.
```{r "3_1__9"}
#< task
#>
show.regression(fit)
```


We observe a positive intercept term $\beta_0$ with 5.87704 and a negative coefficient for $\beta_1$ of -1.30554, meaning that emissions fall with an increasing cost ratio. The three stars next to our coefficents imply, that the p-value is smaller than 0.1. Therefore, our data are inconsistent with the null hypothesis and it may be rejected. In other words the probabiliy of finding an estimator that is at least as high as the one we predicted is smaller than one percent.  

#< info "R-squared"
`R-Squared` is a statistical measurement that represents the correlation between `fitted values` and `observed values`. `R-squared` is hereby always positive and ranges from 0 to 1. A value closer to 1 indicates that the suggested model explains a majority of the variance in the outcome variable.  Mathematically we can descibe this as followed: 
$$R^{2}=1-\frac{Explained\ Variation}{Total\ Variation}$$
A problem with `R-squared` is, that it increases with higher amounts of variables in the model, even if these variables are only barely responsible for the predicted values. `Adjusted R-Squared` takes the number of variables into account, which avoid an false impression and is also shown in the summary statistics.
#>

Next, we determine if the model fits the data. One possibiliy is to use the $R^2$ indicator, which measures quite low at $19.7%$. However, we should take this measurement with caution. As mentioned before, `costratio` is statistically significance, therefore we still can draw conclusions from our results.  


### Multivariate Regression Models

If we look back at the implications of the energy market from `Exercise 1`, we know that there are several other factors that should influence $CO_2$ emissions. To include additional effects in our model we can use `Multivariate Regression Models`. For now, we add the effect of `electricity consumption` or `load`. Our regression formula changes to:  

$$CO_{2t}=\beta_0+\beta_1\cdot costratio_t+\beta_2\cdot load_t+\epsilon_t$$

**Task:** Just press **check**.
```{r "3_1__10"}
fit1 = lm(co2mass ~ costratio + load, data=dat_east)

show.regression(fit,fit1)
```

The influence of `costratio` is around half as big as before. `Load` has a positive coefficient $\beta_2$ of 0.774, which is also logical since emissions increase with more generated electricity. We conclude that the costs of fuel and the generated electricity both have a equal influence in $CO_2$ emissions. According to the p-values, all coefficients are  siginificant. we observe a big difference in the `R-squared` measurement though. In contast to our first model, the second explains a vast majority of the obvserved values With $92.5\%$.  
Since our model is still quite simple, let's pretend we want to use `fit1` further on. However, having a well fitted model is just the first step for us. In the end we want to plot the response curves of emissions against changing costs of fuel. To achieve this, we first have to calculate the fitted values of our model. R provides a function named `predict()`, that does this for us.  

**Task:** Predict the fitted values of `fit1` on `dat_east`.

#< info "Predict"
We can predict the corresponding fitted values of our model with the `predict()` function. At minimum it takes the fitted model and a data frame: predict(`model`, newdata=`data`).
#>

```{r "3_1__11"}
#< task
#>
co2.hat1 = predict(fit1, dat_east)
```

Now, that we calculated the fitted values we could go on and create a plot. A problem we get in this case is, that a multivariate regression with two independent variables spans three dimensions, which isn't sufficient for us in terms of scientific results. To illustrate this problem let's create a three-dimensional graph. Run the code below.

**Task:** Just press **check**
```{r "3_1__12",fig.width=9, fig.height=6, dev='svg'}
#< task_notest
#take sample from data to limit points shown in plot
temp<- dat_east %>% cbind(co2.hat1)
temp1<- temp[sample(nrow(temp), 150), ]
#assign vairables to axises
x <- temp1$costratio
y <- temp1$load
z <- temp1$co2.hat1
#set a nice color scheme
numbercol <- 8 # number of colors
plotcolor <- brewer.pal(numbercol,"Greys") # use Brewer to get color scheme 
colornum <- cut(rank(z), numbercol, labels=FALSE)
colorcode <- plotcolor[colornum] # assign color
#plot
s3d <- scatterplot3d(x,y,z, col.axis="gray", col.grid="gray", type="h",color=colorcode, 
angle = 35, scale.y = 1, pch = 19, xlab="Cost Ratio", ylab = "Load", zlab = "Emissions")
s3d$plane3d(fit1, lty = "dotted")
#>
```

Because of the amount of data points we limit them in our plot to a sample size. The predicted emissions are plotted as dots in the z-axis and the regression plane is placed along side. We see that the plane spans over three dimensions and there no exact "line" to be drawn here. We could pursue this type of visualization but because of clearness we want our results to be plotted on a two-dimensional graph. Since it is not that common to plot regressions this complex, let's see what would happen if we continue and simply plot these predicted values. For purpose of illustration, run the code below.

**Task:** Just press **check**
```{r "3_1__13"}
#< task_notest
ggplot(dat_east,aes(x=costratio,y=co2.hat1))+
        geom_line(aes(x=dat_east$costratio, y=co2.hat1))+
        labs(y="Emissions", x="Cost Ratio")
#>
```

`R` hereby follows it's intential function and tries connects all points, which is not at all the desired output we want to achieve. We could interpret falling emissions for higher cost ratios but a more detailed analysis isn't possible. To solve this issue we will use a work around in the way that we won't predict our values on the original data frame we build our model on. Instead we will evaluate all independent variables with exception of cost ratios at their respective mean (we plot emissions against cost ratios, therefore we evaluate this variable at their true values).  
We are able to do so, because the average of the fitted values $\hat{y_i}$ is equal to the average of the actual values $y_i$:  

$$\frac{1}{n}\sum_{i=1}^n \hat{y_i}=\frac{1}{n}\sum_{i=1}^n y_i$$

This is true for linear regressions with an intercept term (Sarndal, Swensson and Wretman, 1992). As a side note, the sum of the residuals are zero in this case.

To implement this method we create a new data set. Since we only added `load` to our model for now, we include `costratio` and the mean value of `load`. This will serve as our data frame to run `predict()` on. We will once again see how this approach effects our response curves at the end of this exercise.

**Task:** Just press **check**.
```{r "3_1__14"}
#< task_notest
pred_dat = tibble(co2mass = dat_east$co2mass, costratio = dat_east$costratio) %>%
  cbind(load=mean(dat_east$load))
#>
```

### Non-linear models

Now, that we are able to plot multivariate regressions into a two-dimensional grid, we continue to further develop our model. Our previous models assumed a linear relationship between emissions, fuel prices and load. Given the complexity of electricity markets and the statistics we gathered up to this point, we should assume a more complex relationship in form of a highly non-linear response to emissions. The type of regression that fits a non-linear relationship between dependent and independent variables is called `Polynomial Regression`. The easiest way to transform our previous model is to replace the linear variable with a cubic function:

$$CO_{2t}=\beta_0+\beta_1\cdot costratio_t+\beta_2\cdot costratio_t^2+\beta_3\cdot costratio_t^3+\beta_4\cdot load_t+\epsilon_t$$

**Task:** A cubic function is implemented by using `poly()`. We could include all grades of the polynomical function but since `R` does this on it's own, it is enough to define it as followed. Just press **check**.
```{r "3_1__15"}
#< task_notest
fit2 = lm(co2mass ~ poly(costratio,3) + load, data=dat_east)
show.regression(fit1, fit2)
#>
```

The summary shows the statistics for the previous and the polynomial model. The first thing we can observe, is that R provides a coefficient for every grade of the cubic function. Mathematically this function is noted as $-10.059x-0.287x^2+1.444x^3$ and we can calculate the positive turning point with: $\frac{∂(-10.059x-0.287x^2+1.444x^3)}{∂x} = 0 => 2.74058$. This means, that `costratio` has no futher positive effect on emissions when its' value is greater than this number. The value of `R-Squared` is comparable between both models, but as mentioned before we can't purely rely on this measurement. Since we stated that electricity markets should follow a highly non-linear reponse to emissions, we can assume that the non-linear model is a better fit, even though the statistics are comparable.  
We finish this exercise by plotting the reponse curves once again with the methods we introduced before.

**Task:** Run `predict()` on `fit1` and `fit2`. We already predicted values for `fit1`, but this time we use the data frame `pred_dat`.
```{r "3_1__16"}
#< task
#co2.hat1 = ___
#co2.hat2 = ___
#>
co2.hat1 = predict(fit1, pred_dat)
co2.hat2 = predict(fit2, pred_dat)
```

**Task:** Create a new ggplot. Use `geom_line()` to plot one line each for `co2.hat1` and `co2.hat2`. Assign `costratio` to the x-axis and the fitted emission values to the y-axis.
```{r "3_1__17"}
#< task
#ggplot(pred_dat,aes(x=costratio,y=dat_east$co2mass))+
#   geom_line(aes(___), color="red") +
#   geom_line(aes(___), color="green") +
#>
ggplot(pred_dat,aes(x=costratio,y=dat_east$co2mass))+
    geom_line(aes(x=costratio, y=co2.hat1, color="linear")) +
    geom_line(aes(x=costratio, y=co2.hat2, color="non-linear")) +
    scale_color_manual(name="",values = c("black","red"))+
    labs(x="Cost Ratio", y="Emissions")
```

The plot shows us the two regression lines for the linear and non-linear regression approach. With the method of predicting our fitted values to mean values we get a clear trend to interpret our results. Furthermore, the non-linear model seems to reflect the real-world a lot better. Because of technological and capacity restriction it is unlikely that emissions can increase in a linear fashion without restrictions. That effect should also apply for higher cost ratios.  
Looking closely we still see some flaws that don't seem to be accurate. For example it is unlikely that we observe increasing emissions with higher cost ratios. Going forward we will improve the method of fitting polynomial function to our model. Click `Go to next exercise`.

#< award "Regression Expert 1"
TEXT
#>


## Exercise 3.2 -- Restricted Cubic Spline Regressions

In this exercise we will introduce the conecpt of `restricted spline regressions`. In the world of craftman's the term splines would refer to a strip of wood that is being shaped into a smoothed curve. The strip is hereby forced around fixed points to form a natural spline. In the world of mathematics these splines are defined as a form of piece-wise polynomial functions, which usually consist of cubic polynomials. `Knots` refer to the fixed points in mathematical splines and are placed along the range of data. As we ajust the number of knots the spline becomes more or less flexible. Mind here that this can lead to over or underfitting of the fitted curve. A way to confirm the correctness of the fit is to conduct robustness tests where we adjust the number of knots. We will do this briefly at the end of this problem set. At last, our model will be `restricted` , meaning that the spline is constrained to be linear in front of the first knot and after the last one.  

**Task:** To start with spline regression, load the required data. We use the same data and limit them to interconnection `EAST` as before. Just press **check**.
```{r "3_2",message=FALSE}
dat_east <- read_csv("Data/exercise3_2.csv")
```

**Task:** Fit a regression model, which includes `costratio` and `load` as natural splines with `5` degrees of freedom. Use the data frame `dat_east`.  

#< info "ns()" 
`ns()` is part of the `splines` package and allows us to perform natural splines regressions on our data. It takes several input arguments, the only one we will use is `df`. `Df` hereby defines the degrees of freedom and the function chooses `df-1-intercept` knots, which are set at predefined quantiles. In our case we will choose `df=5`, which places our knots at following quantiles: `[0.05, 0.275, 0.5, 0.725, 0.95]` (Harrell, 2001).

A simple syntax example of cubic spline regression: 

```{r "3_2__2",eval=FALSE}
#build a model that tries to explain the relation between weight and color of bananas
fit = lm(banana_weight ~ ns(banana_color, df=5), data=bananas)
```

For full documentation of the function, conduct the help() function.
#>

```{r "3_2__3",message=FALSE}
#< task
#fit3 = lm(co2mass ~ ns(___)+ns(___), data=dat_east)
#>
fit3 = lm(co2mass ~ ns(costratio,df = 5)+ns(load, df=5), data=dat_east)
```

**Task:** Use `show.regression()` to get the summary statistics of `fit3`.
```{r "3_2__4"}
#< task
#>
show.regression(fit3)
```

The summary statistics of our spline model is comparable to the simple polynomial regressions from last exercise, where we got a coefficient for every grade. In this case we get a coefficient for every degree of freedom. Since splines are transformations of explanatory variables it isn't really possible to interpret these coefficients straightforward. Nevertheless we want to get clear and  interpretable results and that's the reason why we went through the trouble and introduced the methods to create plots from our models.

**Task:** Just press **check**.
```{r "3_2__5",warning=FALSE}
#< task_notest
pred_dat = tibble(costratio = dat_east$costratio) %>%
  cbind(load=mean(dat_east$load))
co2.hat3 = predict(fit3, pred_dat)
ggplot(pred_dat,aes(x=costratio,y=dat_east$co2mass))+
    geom_point(alpha=0.3) + 
    geom_line(aes(x=costratio, y=co2.hat3), color="red") + 
    scale_y_continuous(limits=c(4,6))
#>
```

We continue to see a negative relationship between cost ratios and $CO_2$ emissions. Additionally I added the data points of $CO_2$ emissions. As cost ratios increase, the difference between our regression line and the observations tend to become larger. This is a strong indicator that our model suffers from heteroskedasticity. Since standard error methods rely on the assumption that there's no correlation between the independent variables and the variance of the dependent variables, we calculate `heteroskedasticity robust standard errors`.  
R packages normally provide a lot of handy functions to predict fitted values and calculate robust standard errors at the same time. However, we don't predict our fitted values on the same data frame we build our model on. Therefore this will be a bit tricky. We will use our own custom function that is based on the Newey-West estimator `NeweyWest()`, which is part of the `sandwich` package. This will enable us to add confidence intervals to our plot later on. For reference I included the function below, however we won't go through the theory or explain the function in depth.  

#< info "Custom function for Newey-West Standard Errors" 

```{r "3_2__6",eval=FALSE}
Newey <- function(model, predict) {
  temp <- NeweyWest(model, newdata=predict)
  X_mat <- model.matrix(model)
  var_fit_hac <- rowSums((X_mat %*% temp) * X_mat)
  se_fit_hac = sqrt(var_fit_hac)
  
  pred_df <- pred_df %>%
  mutate(se_fit_hac = sqrt(var_fit_hac)) %>%
  mutate(
    lwr_hac = pred_df$fit - 1.96*var_fit_hac,
    upr_hac = pred_df$fit + 1.96*var_fit_hac
  )
}
```

#>

#< award "Sp(l)ine Surgeon"
TEXT
#>



## Exercise 4 -- Examining Carbon Abatement


In the following chapter we shift our focus away from theory and work our way towards our final results. To approach our main analysis we split it into two parts:

- First, we determine the $CO_2$ response curves to changing fuel costs. To reflect the energy market as good as possible we will add additional factors to the model we introduced in the previous exericise. (`Exercise 4.1`)

- Afterwards, we will, based on the theory of Exercise 2, transform our fuel prices into carbon prices and discuss the final results. (`Exercise 4.2`)

In Chapter 3 we introduced methods to create a good model and to keep it simple we limited our data to fuel prices and generated electricity (`load`). In this exercise we will introduce additional factors to our data frame and get a final impression of what we are working with. 

**Task:** Load the data set `main_data.csv` with `read_csv` and store it into a variable called `dat`.

```{r "4",message=FALSE}
#< task
#___ <- ___("./Data/___")
#>
dat <- read_csv("./Data/main_data.csv")
```
&nbsp;
The data frame is a combination of data we used before and in adds several other factors that have a influence on emissions. The variables we add are explained below:

`tlsd`, `tlmin`, `tlmax`: Standard deviation, minimum and maximum of $load$ 
`meant`: Average daily temperature  
`nonFossil`: Electricity generated with non-fossil fuel in MWh  
`so2price`: Permit prices of $SO_2$ ($/ton)  
`netNSflow`: Electricity flowing from Canada to the U.S. in MWh  

The data are gathered from several official U.S. agencies and continue to be aggregated to a daily level and seperated by interconnection. The U.S. Energy Information Administration (EIA) provides data for `non-fossil` energy production, permit prices for $SO_2$ are collected from the `EPA Clean Air Markets` and finally, `net imports` of electricity from Canada are gathered from the `National Energy Board of Canada`. You can find links to the links to the data in the `References` section.

Another factor we haven't considered yet is the seasonal component in energy consumption. We already saw in `Exercise 1`, that load heavily varies during the year with a smaller peak in summer and a bigger one in winter. To adjust for this effect we create a dummy variable that controls for years and seasons (off-season/summer/winter). 

**Task:** Calculate the seaonal variable and show the first few rows. Just press *check*.
```{r "4__2"}
dat_final <- dat %>% 
        mutate(season=(month>3) + (month>6) + (month>9),
                       yearseason=year*10+season)

head(dat_final,3)
```

Now that we included all variables we wanted, we will use this data set to run our main analysis on. We already got a glimpse on the data earlier in this problem set. Now that we have our complete data we will create a summary table, which will display the means of important variables and seperates them for each interconnections. That way we can draw some important conclusions for further interpretations.

**Task:** We use `group_by()` to group the dataset `dat` by `intercn`. Afterwards, run `summarise_all()` to calculate the means of every column. For formatting we use the `kable`. Just press **check**.

#< info "Group_by() and summarise()"
`Group_by()` allows you to group a data frame by specific variables. Operations that are run on the grouped data frame are then performed on each group.

The function `Summarise()` runs on **grouped data** and can perform operations e.g. calculating means (`mean()`) or finding minimums (`min()`). There are several
pre-implemented version of `summarise` functions in `R`. The one we use here is `summarise_all()`, which performs these operations on all columns.

To give you an example in code form, lets pretend we have a dataframe `data` with several `car manufacturer` and their respective car models with `prices` and we want to calculate the average car price per manufacturer.

```{r "4__3",eval=FALSE}
example <- data %>% 
  group_by(manufacturer) %>%
  summarise(mean_price = mean(price))
```

Call `help(group_by)` or `help(summarize)` for further information.
#>
  
#< info "kable()"
The package `kableExtra` and in particular the function `kable()` provides us with a framework to build complex LaTeX tables and enables us to manipulate the table styles by adding html/css elements. It supports the use of the pipe function `%>%` and works quite similar to `ggplot` by allowing us to add layers to our tables.

You can find detailed information on the functionalities [here](http://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf).
#>

**Task:** Just press **check**.
```{r "4__4"}
#< task_notest
dat_final %>% 
  select(intercn, co2mass, load, coalprice, gasprice, costratio, nonfossil) %>% 
  group_by(intercn) %>% 
  mutate(co2mass = co2mass/1000, load=load/1000, "Emission Rate"=co2mass/load, nonfossil=nonfossil/100000) %>% 
  summarise_all("mean") %>% 
  kable(format="html", align="c", col.names = c("intercn","Emissions","Load","Gas Price","Coal Price","Price Ratio","Non Fossil","Emission Rate")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), position = "center", full_width = F)
#>
```

The table reports the mean of important variables for each interconnection. We can clearly see that `EAST` is by far the largest of the observed energy markets. The emissions seem to increase proportionally to the electricity consumption if we take the electricity production from non-fossil sources in consideration. This gets clearer if the look at the emission rates, which are defined as $ER=\frac{emissions}{load}$. Therefore the `Western` interconnection has by far the `greenest` energy production, followed with some distance by Texas (`ERCOT`) and the `Eastern` interconnection. Furthermore we observe clear variations in price ratios.   This is in line with our assumptions from `Chapter 2`, where we stated that each interconnection has vastly different conditions and therefore should conduct our regression on each seperatly. 

In the next exercise we will use this data set and the regression theory we presented in `Chapter 3` to trace out the emission response curves.

#< award "Data Expert"
You know everything about our data. You are ready for the analysis.
#>

Click `Go to next exercise` to continue.

## Exercise 4.1 -- Estimated CO2 Response to Fuel Prices

In this exercise we expand the regression model from the previous Chapter and determine the $CO_2$ response curves to changing fuel prices. In the first step we will perform a regresson based on the methods we introduced in the previous chapter. Afterwards we will use this model to predict emissions and plot a response curve that gives us percentage changes based on predicted fuel prices of 2025.

As a side note, the code isn't meant to be the shortest or the most efficient, but should allow you to follow each step we take to get our final results. Based on this we will explain and carry out the analysis step by step for interconnection `EAST`, afterwards apply it to `ERCOT` and `WECC` and interpret our results.

Building upon the model we defined in the last chapter, we will expand our regression model with several control variables as followed. The meaning and source of data to every variable is explained in the info box below:

First, we load and prepare the data to run the ///////// SEASONAL VARIABLE

**Task:** Load the dataframe and store it in `dat`. create the seasonable dummy variable. 
```{r "4_1",message=FALSE}
dat <- read_csv("Data/main_data.csv") %>% 
        mutate(season=(month>3) + (month>6) + (month>9),
                       yearseason=year*10+season)
head(dat,3)
```

**Task:** Filter the data set for intercn `EAST`.
```{r "4_1__2"}
#< task
#east <- filter(...,...)
#>
east <- filter(dat, intercn=="EAST")
#< hint
display("Filter() takes a data frame as first argument and a logic as second. We want to filter for intercn EAST.")
#>
```

**Task:** Calculate the `mean` of every variable we use in our regression. First, select the necessary columns and then use `summarise_all` to get the `mean`. Store the result in `mean_east`.
```{r "4_1__3"}
#< task
# ... <- east %>% 
#  select(load, tlsd, tlmin, tlmax, meant, nonfossil, so2price, netNSflow, yearseason) %>% 
#  summarise_all(...)
#>
mean_east <- east %>% 
  select(load, tlsd, tlmin, tlmax, meant, nonfossil, so2price, netNSflow, yearseason) %>% 
  summarise_all(mean)
#< hint
display("In this case, you can insert the mathematical operation directly into summarise_all().")
#>
```

**Task:** To make our life a bit easier we will create a new data set with `gasprice`, `priceratio` and the results of the `last` task. Use `tibble()`, which creates a new data frame and `cbind()`, which takes a sequence of columns and combines them with another data frame.

```{r "4_1__4"}
#< task
#temp_east <- ...(gasprice = east$gasprice, priceratio = east$priceratio) %>% 
#  ...(mean_east)
#>
predict_east <- tibble(date=east$date, intercn=east$intercn, co2mass=east$co2mass, gasprice = east$gasprice, coalprice=east$coalprice, costratio = east$costratio) %>% 
  cbind(mean_east)
```


#We will perform a reduced-form regression which expands the cubic spline regression from exercise `3.2` and builds upon the theory of `exercise 2`. We define the model as followed: ??????????

$$\tag{5}CO_{2t}=s(priceratio_{t}|\beta)+s(load_{t}|\theta) + s(temp_t|\omega)+X_t\psi+D_\gamma+\epsilon_t$$

#< info "Model variables" 
We already introduced the meaning of some variables in previous exercises. To get one complete view I listed them below again: ?????????
$CO_{2t}$: CO2 emissions in tons  
$priceratio_{t}$: Cost ratio of coal over gas
$load_{t}$: daily electricity consumption per interconnection in MWh  
$temp_{t}$: average daily temperature per interconnection  
$X_{t}$: Factors like non-fossil electricity production (e.g. solar, hydro or wind), $SO_{2}$ price, net imports of electricity from Canada and variance in load  
$D_\gamma$: Dummy variable for seasonal variation to absorb fluctuations, e.g. by renewable energies.
#>



**Task:** Perform the regression model we described in the beginning of this exercise for interconnection `EAST`. Use `ns()` for variables with spline regression (as explained in `3.3`). We want to use `5` degrees of freedom. Store the resulting model in `reg_east`.
```{r "4_1__5"}
#< task
#... <- lm(co2mass ~ ...(priceratio, df=...) + ...(load, df=...) + tlsd + tlmin + tlmax + ...(meant, df=...) + nonfossil + so2price + netNSflow + yearseason, data=east)
#>
reg_east <- lm(co2mass ~ ns(costratio, df=5) + ns(load, df=5) + tlsd + tlmin + tlmax + ns(meant, df=5) + nonfossil + so2price + netNSflow + yearseason, data=east)
```

**Task:** Predict $CO_2$ emissions based on the regression model `reg_east` and `temp_east`. We set interval to `confidence` to get the mean interval and be able to plot a confidence band later on.
Just press *check*.

#< info "Confidence interval" 
A confidence interval answers the question for which defined probability the data points lie within the interval. Mathematically, given we have observations $x_1...x_n$ and a confidence level $\gamma$, a confidence interval has a probability $\gamma$ to contain the true underlying parameter. Most commonly, and also in our case, we use
the 95% confidence interval and is defined as: 

$$\hat{y_h} ± t_{\alpha/2,n-2} \sqrt{MSE \frac{1}{n}+\frac{(x_k-\overline{x})^2}{\sum x_i-\overline{x}^2)}}$$
where $\hat{\gamma}$ is the fitted response, $t_{\alpha/2,n-2}$ the t-value with n-2 degrees of freedom and the equation inside the square root represents the standard error.
#>




/// reg_robust <- lm_robust(y1 ~ x, mydat, se_type = 'stata')
/// https://ditraglia.com/econ224/lab07.pdf





```{r "4_1__6"}
#< task_notest
fit_east <- predict_east %>% 
  cbind(as.data.frame(predict(reg_east, newdata = predict_east, interval = 'confidence')))
#>
```

**Task:** Just press *check*.
```{r "4_1__7"}
basecoal=2.25
basegas=5.75

diff_base <- abs(east$costratio - (basecoal/basegas))
min_dif <- min(diff_base)
closest <- min_dif==diff_base
base_emit <- mean(fit_east$fit[which(closest)])

final_east <- fit_east %>% 
  mutate(fit.transformed=fit/base_emit-1,
         lwr.transformed=lwr/base_emit-1,
         upr.transformed=upr/base_emit-1)
```

**Task:** Just press *check*.
```{r "4_1__8",warning=FALSE, fig.width=7, fig.height=7}
#< task_notest
plot_east <- ggplot(final_east, aes(x=basecoal/costratio, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = final_east$lwr.transformed, ymax = final_east$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5,) +
  geom_line(aes(y=fit.transformed)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-.15,.05)) +
  geom_vline(xintercept = 5.75, linetype = "dashed") + 
  scale_x_continuous(breaks=c(2,3,4,5,6,7,8,9,10,11,12), limits=c(2.2, 12)) +
  labs(title="Eastern Interconnection", subtitle = "y=Percentage Change in CO2 emissions",
       y="", x="Gas Price $/MMBTU") +
  theme_minimal()

plot_east
#>
```

TEXT////



**Task:** Run the code for the `ERCOT` interconnection. Just press *check*.
```{r "4_1__9",warning=FALSE, fig.width=7, fig.height=7}
#< task_notest

#filter for interconnection ERCOT
ercot <- filter(dat, intercn=="ERCOT")

#calculate the mean for regression coefficients
mean_ercot <- ercot %>% 
  select(load, tlsd, tlmin, tlmax, meant, nonfossil, so2price, netNSflow, yearseason) %>% 
  summarise_all(mean)

#join data
predict_ercot <- tibble(date=ercot$date, intercn=ercot$intercn, gasprice = ercot$gasprice, coalprice=ercot$coalprice, priceratio = ercot$priceratio) %>% cbind(mean_ercot)

#run model 
reg_ercot <- lm(co2mass ~ ns(priceratio, df=5) + ns(load, df=5) + tlsd + tlmin + tlmax + ns(meant, df=5) + nonfossil + so2price + netNSflow + yearseason, data=ercot)

#predict new co2 emission and join with predict_ercot, ERCOT has no net imports of electricity, predict throws warning
fit_ercot <- predict_ercot %>% 
  cbind(as.data.frame(predict(reg_ercot, newdata = predict_ercot, interval = 'confidence')))

#find baselevel of emission in relation to baseline price of coal and gas
diff_base <- abs(ercot$priceratio - (basecoal/basegas))
min_dif <- min(diff_base)
closest <- min_dif==diff_base
base_emit <- mean(fit_ercot$fit[which(closest)])

#transform predicted emission into percentage change to baseline emissions
final_ercot <- fit_ercot %>% 
  mutate(fit.transformed=fit/base_emit-1,
         lwr.transformed=lwr/base_emit-1,
         upr.transformed=upr/base_emit-1)

#plot result
plot_ercot <- ggplot(final_ercot, aes(x=basecoal/priceratio, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = final_ercot$lwr.transformed, ymax = final_ercot$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5) +
  geom_line(aes(y=fit.transformed)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-.15,.05)) +
  geom_vline(xintercept = 5.75, linetype = "dashed") + 
  scale_x_continuous(breaks=c(2,3,4,5,6,7,8,9,10,11,12), limits=c(2.2, 12)) +
  labs(title="ERCOT Interconnection", subtitle = "y=Percentage Change in CO2 emissions",
       y="", x="Gas Price $/MMBTU") +
  theme_minimal()
#>
```

**Task:** Run the code for the `WECC` interconnection. Just press *check*.
```{r "4_1__10",warning=FALSE, fig.width=7, fig.height=7}
#< task_notest
wecc <- filter(dat, intercn=="WECC")

mean_wecc <- wecc %>% 
  select(load, tlsd, tlmin, tlmax, meant, nonfossil, so2price, netNSflow, yearseason) %>% 
  summarise_all(mean)

predict_wecc <- tibble(date=wecc$date, intercn=wecc$intercn, gasprice = wecc$gasprice, coalprice=wecc$coalprice, priceratio = wecc$priceratio) %>% cbind(mean_wecc)

reg_wecc <- lm(co2mass ~ ns(priceratio, df=5) + ns(load, df=5) + tlsd + tlmin + tlmax + ns(meant, df=5) + nonfossil + so2price + netNSflow + yearseason, data=wecc)

fit_wecc <- predict_wecc %>% 
  cbind(as.data.frame(predict(reg_wecc, newdata = predict_wecc, interval = 'confidence')))

base_emit <- mean(fit_wecc$fit[which(min(abs(wecc$priceratio - (basecoal/basegas)))==abs(wecc$priceratio - (basecoal/basegas)))])

final_wecc <- fit_wecc %>% 
  mutate(fit.transformed=fit/base_emit-1,
         lwr.transformed=lwr/base_emit-1,
         upr.transformed=upr/base_emit-1)

plot_wecc <- ggplot(final_wecc, aes(x=basecoal/priceratio, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = final_wecc$lwr.transformed, ymax = final_wecc$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5) +
  geom_line(aes(y=fit.transformed)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-.15,.05)) +
  geom_vline(xintercept = 5.75, linetype = "dashed") + 
  scale_x_continuous(breaks=c(2,3,4,5,6,7,8,9,10,11,12), limits=c(2.2, 12)) +
  labs(title="Western Interconnection", subtitle = "y=Percentage Change in CO2 emissions",
       y="", x="Gas Price $/MMBTU") +
  theme_minimal()
#>
```

**Task:** Use grid.arrange() to display the plots next to each other. Just press *check*.

```{r "4_1__11",warning=FALSE, fig.width=14, fig.height=7}
grid.arrange(plot_east, plot_ercot, plot_wecc,  nrow=1)
```


//TEXT, east oben, hier verlgeich zu ercot, wecc






///////// test
```{r "4_1__12"}

main_data2 <- rbind(final_east, final_ercot, final_wecc)
main_data2 <- main_data2[order(as.Date(main_data2$date, format="%d/%m/%Y")),]
write.csv(main_data2, "X:\\libraries\\RTutor_BA\\main_data2.csv", row.names = FALSE)
temp <- read_csv("main_data2.csv")
write.csv(dat_east, "X:\\libraries\\RTutor_BA\\exercise3_2.csv", row.names = FALSE)
```




#< award "Sp(l)ine Surgeon2"
This was a critical operation, but you mastered it! Keep going, the hardest part is over.
#>

In the next exercise we will map the emission response curves from this exercise into carbon prices and estimate the effects of carbon prices on emissions as well as the associated costs. Click `Go to next exercise` to continue.


## Exercise 4.2 --  Imputed CO2 Response to Carbon Prices

In the last exercise we found out that changing cost ratios of `coal` and `gas` could greatly reduce $CO_2$ emissions. What we are interested though is the question how carbon prices effect emissions. This exercise will follow the analysis of Cullen and we transform the cost ratios into `carbon prices`. This will allow us to estimate the effect of carbon taxes on $CO_2$ emissions and we can approximate the costs of such reductions. We use `Equation 3` to transform the prices and afterwards plot and calculate our final results based on the model we defined in `Exercise 4.1`.

We assume the same fixed and predicted fuel prices for 2025 as before and additionally define the fuels associated carbon contents (EIA 2019):

- Average delivered coal price `$2.25/mmBTU` and gas prices `$5.75/mmBTU` for 2025. 
- Carbon content `Natural Gas`: 117 lbs carbon/MMBTU or `0.0585 tons/MMBTU`
- Carbon content `Coal`: 210.8 lbs carbon/MMBTU or `0.1054 tons/MMBTU` (averaged on weighted fuel consumption according to EIA Form 923)

To avoid repeating the same steps as in the last exercise, I prepared a data set that includes the predicted `emissions` as well as the `tranformed emissions` with confidence intervals. Load the data set `main_data2.csv` and store it in `dat`. Use the `head()` command to show the first rows. We also declare the variables for `base prices` and `carbon content`.

**Task:** Just press *check*.
```{r "4_2",message=FALSE}
#< task_notest
dat <- read_csv("Data/main_data2.csv")
head(dat,3)

basegas <- 5.75
basecoal <- 2.25
gas_cc <- 0.0585
coal_cc <- 0.1054
#>
```

In the next step we go ahead and transform the cost ratios into carbon prices using `Equation 3`. For reference I included it below once again. Based on the method and cost ratios in our data we also get negative carbon taxes after the transformation. Since it doesn't make much sense for use to consider these, we will filter them out. For reference or when you decided to skip to this Exercise I once again include `Equation 3`:  

$$\tag{3}{P_{co2}}=\frac{CR\cdot{P_{gas}}-{P_{coal}}}{CO_{2,coal}-CR\cdot CO_{2,gas}}$$

**Task:** Create a new column `carbontax` using `Equation 3` and filter the just calculated taxes for the interval [0,80]. Save the result into data frame `tax` and display the first rows.
```{r "4_2__2",message=FALSE}
#< task
#... <- dat %>% 
#  mutate(carbontax = (priceratio*...-...)/(coal_cc-priceratio*gas_cc)) %>% 
#  filter(carbontax >= ... & carbontax <= ...)
#
#head(...,...)
#>
tax <- dat %>% 
  mutate(carbontax = (priceratio*basegas-basecoal)/(coal_cc-priceratio*gas_cc)) %>% 
  filter(carbontax >= 0 & carbontax <= 80)

head(tax,3)
#< hint
display("Following the equation given above to calculate cabon taxes. We want to keep carbon tax rates between 0 and 80, simply insert the numbers. Display the first three rows with head(dataframe, 3).")
#>
```

The overall approach of plotting our results is similar to the one in Exercise `4.1`. We will filter for each interconnection seperatly and plot our result with `ggplot2`. First we will create a plot for interconnection `EAST` and interpret the results. Afterwards we will run the code on the other interconnections and compare them to another. In contrast to the previous Exercise though, we won't plot the emission change based on changing cost ratios, but obviously on changing `carbon prices`.

**Task:** Filter the data frame `tax` for interconnection `EAST` and save it to `tax_east`. Since we filter for a string, don't forget to put quotes around the argument.
```{r "4_2__3"}
#< task
#>
tax_east <- filter(tax, intercn=="EAST")
#< hint
display("Use filter(dataset,filter). We want to filter for interconnection EAST.")
#>
```

**Task:** Just press *check*.
```{r "4_2__4",fig.width=7, fig.height=7}
#< task_notest
plot_east <- ggplot(tax_east, aes(x=carbontax, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = tax_east$lwr.transformed, ymax = tax_east$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5) +
  geom_line(aes(y=fit.transformed)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-0.15,.01)) +
  scale_x_continuous(limits=c(0, 80)) +
  labs(title="Eastern Interconnection",
       y="", x="Carbon Price $/ton", subtitle="y=CO2 emissions in Percentage")

plot_east
#>
```

Our first plot shows the reaction of emission with the introduction of carbon prices in the Eastern Interconnection . $CO_2$ emissions react steeper with lower carbon prices. We will get to exact numbers at the end of this exercise, but for now we can argue that a majority of emission reduction can be achieved with a mediocre level of carbon prices. 



Run the code chunks below to create the plot for the other interconnections and display them all together.

**Task:** Run the code for the `ERCOT` interconnection. Just press *check*.
```{r "4_2__5",fig.width=7, fig.height=7}
#< task_notest
tax_ercot <- filter(tax, intercn=="ERCOT")

plot_ercot <- ggplot(tax_ercot, aes(x=carbontax, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = tax_ercot$lwr.transformed, ymax = tax_ercot$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5) +
  geom_line(aes(y=fit.transformed)) +  
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-0.15,.01)) +
  scale_x_continuous(limits=c(0, 80)) +
  labs(title="Ercot Interconnection",
       y="", x="Carbon Price $/ton", subtitle="y=CO2 emissions in Percentage")
#>
```

**Task:** Run the code for the `WECC` interconnection. Just press *check*.
```{r "4_2__6",fig.width=7, fig.height=7}
#< task_notest
tax_wecc <- filter(tax, intercn=="WECC")

plot_wecc <- ggplot(tax_wecc, aes(x=carbontax, y=fit.transformed)) +
  geom_ribbon(aes_string(ymin = tax_wecc$lwr.transformed, ymax = tax_wecc$upr.transformed),
                   colour="lightgrey", fill="lightgrey", alpha=0.5) +
  geom_line(aes(y=fit.transformed)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits=c(-0.15,.01)) +
  scale_x_continuous(limits=c(0, 80)) +
  labs(title="Western Interconnection",
       y="", x="Carbon Price $/ton", subtitle="y=CO2 emissions in Percentage")
#>
```
&nbsp;
#< quiz "Compare emission abatement"
question: We saw earlier that interconnection `ERCOT` has a higher generation of renewable energies than `EAST`. Do you expect `ERCOT` to have a higher or lower emission abatement in comparison to `EAST`?

sc:
    - higher
    - lower*
success: Great, your answer is correct!
failure: Try again.
#>  

&nbsp;  
  
**Task:** Just press *check*.
```{r "4_2__7",fig.width=21, fig.height=7}
grid.arrange(plot_east, plot_ercot, plot_wecc, nrow=1)
```

TEXT

Because of the fact that ercot is already more reliant on renewable energies we can argue that the emission reduction from switching between gas and coal- 
is steepest with a introduction of carbon prices and slows afterwards.


Based on our method we dont get carbon prices higher than 72 for West because of the price structure.
In the end all interconnection end up at around a reduction of 12% 

We can argue that additional carbon reduction will result from other factors if 

#< award "2?"
?
#>


**Task:** Just press *check*.
```{r "4_2__8"}
#< task_notest
find_closest_value <- function(a) {
  vector <- 1:9
  for(i in 0:8){
   vector[i+1] = mean(a$fit[which.min(abs(a$carbontax - i*10))])
  }
  return(vector)
}
temp1 = data.frame(tax=seq(0,80,10),east_emission=round(find_closest_value(tax_east)/100000,1), ercot_emission=round(find_closest_value(tax_ercot)/100000,1), wecc_emission=round(find_closest_value(tax_wecc)/100000,1))

temp2 <- temp1 %>% 
  mutate(perc_east=round((1-temp1$east_emission/temp1$east_emission[1])*100,1),
         perc_ercot=round((1-temp1$ercot_emission/temp1$ercot_emission[1])*100,1),
         perc_wecc=round((1-temp1$wecc_emission/temp1$wecc_emission[1])*100,1),
         emission_all=east_emission+ercot_emission+wecc_emission)

temp3 <- temp2 %>% 
  mutate(perc_all=round((1-temp2$emission_all/temp2$emission[1])*100,1))
  
table <- temp3 %>%
  select(c(1,2,5,3,6,4,7,8,9)) %>%
  kable(format="html", col.names = c("Tax","abs.","%","abs.","%","abs.","%","abs.","%"), align="c", caption = "Precited Emissions and Percentage Abatement") %>%
  add_header_above(c(" "=1, "East" = 2, "ERCOT" = 2, "West" = 2, "Total" = 2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), position = "center", full_width = F)%>%
  #column_spec(1:9, width = "0.4") %>%
  footnote(general = "Predicted emission are in 100.000 tons/day. Change to baseline (Tax=0).")

table
#>
```


The result corresponds to Table 2 in Cullen. 

TEXT



From the results of the table we can additionally derive the costs for such a measure. Let's pretend we want to introduce a carbon tax of $30 with total daily reductions of 5.7%. This would cost us approx. 

```{r "4_2__9"}
days=365
costs_30 = (64.8-(64.8-61.1))/100000*30*days
costs_30
```

Therefore it would cost around 183 millions dollars to 

```{r "4_2__10"}
costs_60 = (64.8-58.5)*100000*60
costs_60
```













To conclude our analysis we can calculate the abatement costs for certain carbon taxes. Lets say we introduce a carbon tax of `40$/ton`

```{r "4_2__11"}

```

Calculating costs






Concluding, our results seem to be robust  to parameter changes.

## Exercise 5 -- Conclusion

Along this problem set we discovered that we are able to reduce carbon emission by a good margin with the introduction of carbon taxes.





Since today there are coal generators that are quite effecient, a carbon tax as low as $20/ton would be efficient to carbon heavy plants (Cullen & Renolds, 2016)






I hope you enjoyed working through this problem set. 

Below you can get an overview of the awards you earned along the way.

```{r "5"}
#< task
awards()
#>
```


The methods used here can be further used to generate additoinal results. This could be a point where you can go on.

https://www.deraktionaer.de/artikel/mobilitaet-oel-energie/royal-dutch-shell-das-waere-der-hammer--20198595.html


## Exercise 6 -- Bonus - Robutness Tests

//// einschub robustness test mit anderen definition von priceratios , appendix a2< number of knots NUR FUER EAST, danach kurz interpretieren

nur plots, kurz vergleichen

#! start_note "Robustness Test - andere priceratio"

```{r "6",optional=TRUE}
#< task
# show that all integers between 0 and 10
#>
1:10
```

#! end_note 

#! start_note "Robustness Test - weniger knots"

```{r "6__2",optional=TRUE}
#< task
# show that all integers between 0 and 10
#>
1:10
```

#! end_note 



## Exercise References


### Bibliography

- Bushnell, J. B., Mansur, E. T. & Saravia, C. (2008), ‘Vertical arrangements, market structure, and competition: An analysis of restructured US electricity markets’, American Economic Review 98(1), 237–66.

- Cullen, Joseph A., and Erin T. Mansur. 2017. "Inferring Carbon Abatement Costs in Electricity Markets: A Revealed Preference Approach Using the Shale Revolution." American Economic Journal: Economic Policy, 9 (3): 106-33.

- Davis, L. & Hausman, C. (2015), Market impacts of a nuclear power plant closure, Ei @ haas working paper wp-248.

- Harrell, F. (2012), ‘Regression Model Strategies‘, Springer-verlag.

- Lafrancois, B. A. (2012), ‘A lot left over: Reducing CO2 emissions in the United States’ electric power sector through the use of natural gas’, Energy Policy 50, 428–435.

- Mansur, E. T. & White, M. (2012), ‘Market organization and efficiency in electricity markets’, (Working Paper).

- Swensson, Wretman (1992), "Model Assisted Survey Sampling", Springer-Verlag.

### R Packages

- Auguie, B. (2017): gridExtra: "Functions in Grid graphics", R package version 2.3, http://cran.r-project.org/web/packages/gridExtra/index.html

- Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer 

- Kranz, S. (2020): RTutor. "Creating R problem sets with automatic assessment of student's solutions", R package version 2020.03.13, https://github.com/skranz/RTutor

- Ligges, U., Maechler, M., Schnackenberg, S. (2018), scatterplot3d: "Plots a three dimensional (3D) point cloud.", R package version 0.3-41, https://cran.r-project.org/web/packages/scatterplot3d/index.html

- Neuwirth, E. (2014), RColorBrewer: "Provides color schemes for maps (and other graphics)". R package version 1.1-2, https://cran.r-project.org/web/packages/RColorBrewer/index.html

- Rudis, B. (2017): ggalt: "Extra Coordinate Systems, 'Geoms', Statistical Transformations, Scales and Fonts for 'ggplot2'", R package version 0.4.0, https://cran.r-project.org/web/packages/ggalt/index.html

- Venables ,W., Bates, D. (2019), splines: "Regression spline functions and classes.", R package version 3.6.2, Part of R.3.6.2

- Wickham, H. (2016): ggplot2. "Elegant Graphics for Data Analysis", Springer-Verlag, New York, R package version 3.2.1, http://CRAN.R-project.org/package=ggplot2

- Wickham, H., Francois, R., Henry, L., Muller, K., (2018): dplyr. "A Grammar of Data Manipulation", R package version 0.8.3, http://CRAN.R-project.org/package=dplyr

- Zhu, H. (2019), kableExtra: "Construct Complex Table with 'kable' and Pipe Syntax", R package version 1.1.0, https://cran.r-project.org/web/packages/kableExtra/index.html

### Data Sources

- European Environment Agency, "Primary Energy Consumption by Fuel", https://www.eea.europa.eu/data-and-maps/indicators/primary-energy-consumption-by-fuel-6/assessment-2

- Federal Energy Regulatory Comission, "Form  No. 714 Annual Electric Balancing Authority Area
and Planning Area Report", https://www.ferc.gov/docs-filing/forms/form-714/data.asp

- Government of Canada, "Canada Energy Regulator", https://www.cer-rec.gc.ca/bts/ctrg/gnnb/lctrctxprts/index-eng.html

- Intercontinental Exchange, "Commodity Prices", https://www.theice.com/marketdata/reports

- National Centers for environmental Information (NOAA), Climate data, https://www.ncdc.noaa.gov/cag/statewide/time-series

- The World Bank , "Commodity Markets Monthly Prices",  https://www.worldbank.org/en/research/commodity-markets

- United States Environmental Protection Agency, "SO2 Trading Program", https://ampd.epa.gov/ampd/

- U.S. Energy Information Administration, "FAQ", (https://www.eia.gov/tools/faqs/faq.php?id=73&t=11)

- U.S. Energy Information Administration, "Form EIA-923", https://www.eia.gov/electricity/data/eia923/

- U.S. Energy Information Administration, "Today in Energy", https://www.eia.gov/todayinenergy/detail.php?id=43035

*All of the above links were accessable as of March 31, 2020.*

